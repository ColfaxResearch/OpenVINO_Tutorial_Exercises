{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Benchmarking\n",
    "\n",
    "In this exercise, you will implement benchmarking code for testing performance.\n",
    "\n",
    "The workload will be the same vehicle detection code as exercise 1. \n",
    "But in this exercise, you will be repeating the process and timing it.\n",
    "\n",
    "## Step 1: Converting the Model\n",
    "\n",
    "You will be using the `mobilenet-ssd` model again for the benchmark code.\n",
    "\n",
    "Use the moel downloader and model optimizer to generate a model for **FP16**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Download and convert mobilenet-ssd model for FP16\n",
    "/opt/intel/openvino/deployment_tools/tools/model_downloader/downloader.py --print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Inference Benchmark Scripts\n",
    "\n",
    "In this step, you will be writing the benchmarking code for testing various hardware available to you on the DevCloud.\n",
    "The hardware that you wil test include those that perform best when there aremore than one request.\n",
    "So as discussed in the video, we need to take advantage of the asynchronous inference mode to spawn the optimial number of inference requests.\n",
    "\n",
    "For this exercise, you will be jumping straight to creating scripts for running the workload in the queue.\n",
    "The scripts will be in two parts: `utils.py` where the helper functions like the function for peprocessing image are located, and `main.py` where the benchmarking occurs.\n",
    "\n",
    "### utils.py\n",
    "\n",
    "Follow the instructions to complete `utils.py`.\n",
    "\n",
    "*(2.1)* Complete the `prepImage()` function, which is used to prepare the image for inference. The code here should be the exact same as in exercise 1.\n",
    "\n",
    "*(2.2)* In `createExecNetwork()` function, create an instance of IECore object and load the CPU plugin specified by the variable `extension`. The solution is identical to the implementationin exercise 1. Optionally, add a if check to see if \"CPU\" appears in the device list. While it is safe toload the extension even if you are not using CPU, it is a good practice to add a check to not load unnecessary extensions.\n",
    "\n",
    "*(2.3)* In `createExecNetwork()` function, create an instance of ExecutableNetwork from `ie_net` with the optimal number of requests and return it. Remember that you must create ExecutableNetwork object (with default `num_request`) first to get the optimal number of requests. See the slides for video 2 of course 2 for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile utils.py\n",
    "import cv2\n",
    "from openvino.inference_engine import IECore, IENetwork\n",
    "\n",
    "def prepImage(original_image, ie_net):\n",
    "\n",
    "    ##! (2.1) Find n, c, h, w from net !##\n",
    "    \n",
    "    # Reshaping data\n",
    "    input_image = cv2.resize(original_image, (w, h))\n",
    "    input_image = input_image.transpose((2, 0, 1))\n",
    "    input_image.reshape((n, c, h, w))\n",
    "\n",
    "    return input_image\n",
    "\n",
    "def getCount(detected_objects, prob_threshold=0.5):\n",
    "    detected_count = 0\n",
    "    for obj in detected_objects[0][0]:\n",
    "        # Draw only objects when probability more than specified threshold\n",
    "        if obj[2] > prob_threshold:\n",
    "            detected_count+=1\n",
    "    return detected_count\n",
    "\n",
    "def createExecNetwork(ie_net, device):\n",
    "    ##! (2.2) Create IECore !##\n",
    "    \n",
    "    ##! (2.2) Load the CPU plugin (optional: check if it is needed)!##\n",
    "    extension = '/opt/intel/openvino/deployment_tools/inference_engine/lib/intel64/libcpu_extension_avx2.so'\n",
    "\n",
    "    ##! (2.3) Create ExecutableNetwork object and find the optimal number of requests !##\n",
    "\n",
    "    ##! (2.3) Recreate IECore and with num_requests set to optimal number of requests !##\n",
    "    \n",
    "    ##! (2.3) return the ExecutableNetwork !##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main.py \n",
    "Next is the `main.py`.\n",
    "For this implementation, follow the approach where preprocessing and postprocessing is also repeated as many time as there are requests.\n",
    "While this is not strictly necessary to repeat the preprocesing and postprocessing steps, it will give you timing that you can directly compare.\n",
    "\n",
    "Follow the instructions to complete `main.py`.\n",
    "\n",
    "*(2.4)* Create the IENetwork object with FP16 version of the `vehicle-detection-adas-0002` model that we have downloaded earlier. Do not change the variable name, `ie_net` for ths file. Then find the name of the input layer and output layer.\n",
    "\n",
    "*(2.5)* Start asynchronous processing on all request slots for images from prepped_images. \n",
    "\n",
    "*(2.6)* Wait for all request slots to complete.\n",
    "\n",
    "*(2.7)* Get the number of vehicles from each inference request with `getCount()` function, and save the result in an array. You will likely need to access the result through the `outputs` attribute of the requests. See slides from course 1 video 7 for more. This array is used for a sanity check to make sure all inference requests return the same number of detected vehicles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile main.py\n",
    "import time\n",
    "from openvino.inference_engine import IENetwork, IECore\n",
    "from utils import *\n",
    "import cv2\n",
    "import sys\n",
    "import os\n",
    "import statistics\n",
    "\n",
    "# Getting the device as commandline argument\n",
    "device = sys.argv[1]\n",
    "\n",
    "##! (2.4) create IENetwork object for vehicle-detection-adas-0002 and set it to ie_net!##\n",
    "ie_net = None\n",
    "\n",
    "##! (2.4) get the input and output layer names !##\n",
    "\n",
    "image_path = \"cars_1900_first_frame.jpg\"\n",
    "original_image = cv2.imread(image_path)\n",
    "\n",
    "iter_ = 500\n",
    "prep_time = []\n",
    "infer_time = []\n",
    "postp_time = []\n",
    "for i in range(iter_):\n",
    "    # Preprocessing image. \n",
    "    prep_start = time.time()\n",
    "    prepped_images = []\n",
    "    for slot_id in range(num_requests):\n",
    "        prepped_images.append(prepImage(original_image, ie_net))\n",
    "    prep_time.append((time.time()-prep_start)/num_requests*1000)\n",
    "    \n",
    "    infer_start = time.time()\n",
    "    for req_slot in range(num_requests):\n",
    "        ##! (2.5) Run asynchronous inference. !##\n",
    "\n",
    "    for req_slot in range(num_requests):\n",
    "        ##! (2.6) Wait for asynchronous inference to complete. !##\n",
    "    infer_time.append((time.time()-infer_start)/num_requests*1000)\n",
    "    \n",
    "    postp_start = time.time()\n",
    "    result_list = [0]*num_requests  # Python way of creating a 0 array of length 'num_requests'\n",
    "    for req_slot in range(num_requests):\n",
    "        ##! (2.7) Run getCount to get the vehicle count and store it in result_list !##\n",
    "        result_list[req_slot] = None\n",
    "    postp_time.append((time.time()-postp_start)/num_requests*1000)\n",
    "\n",
    "    # Sanity check to make sure all results are identical. Abort if it does not match\n",
    "    assert all([x == result_list[0] for x in result_list]), \"Results for the inference requests did not match\"\n",
    "    \n",
    "# writing the results to a file\n",
    "if not os.path.exists(\"results\"):\n",
    "    os.makedirs(\"results\")\n",
    "with open(\"results/{}_val.txt\".format(device), \"w\") as f:\n",
    "    prep_avg    = statistics.mean(prep_time)\n",
    "    prep_stdev  = statistics.stdev(prep_time)\n",
    "    infer_avg   = statistics.mean(infer_time)\n",
    "    infer_stdev = statistics.stdev(infer_time)\n",
    "    postp_avg   = statistics.mean(postp_time)\n",
    "    postp_stdev = statistics.stdev(postp_time)\n",
    "    f.write(\"Inference running on: {} \\n\".format(device))\n",
    "    f.write(\"Number of requests: {} \\n\".format(num_requests))\n",
    "    f.write(\"Inference time per image (ms): {:.3g} +- {:.3g}\\n\".format(infer_avg, infer_stdev))\n",
    "    f.write(\"Preprocessing time per image (ms): {:.3g} +- {:.3g}\\n\".format(prep_avg, prep_stdev))\n",
    "    f.write(\"Postprocessing time per image (ms): {:.3g} +- {:.3g}\\n\".format(postp_avg, postp_stdev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Running Inference Benchmarks\n",
    "\n",
    "With the benchmark scripts in hand you are ready to begin running benchmarks on the DevCloud.\n",
    "The commands for running the job will be provided to you, just like in exercise 1.\n",
    "\n",
    "\n",
    "With that said, there are some differences to note for the job submission in this exercise.\n",
    "In exercise 1, the command to run the job was pushed to the `qsub` through the `echo` command. \n",
    "For this exercise, you will be passing the commands to run for job through a bash script.\n",
    "The reason for this shift is that you will be using an FPGA machine for the benchmarks, and they require an additional step beyond executing `main.py`.\n",
    "As discusses in the videos, FPGAs require \"programs\" in the form of bit-streams to be loaded.\n",
    "For the `mobilenet-ssd` model, OpenVINO has a pre-built bit-stream for it.\n",
    "So the commands have to be added to the bash script, and ran if FPGA is used.\n",
    "\n",
    "### job file\n",
    "\n",
    "Run the following cell to create th bash script `job.sh` to be used for benchmarking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile job.sh\n",
    "\n",
    "# The default path for the job is your home directory, so we change directory to where the files are.\n",
    "cd $PBS_O_WORKDIR\n",
    "DEVICE=$1\n",
    "\n",
    "# Check if FPGA is used \n",
    "if grep -q FPGA <<<\"$DEVICE\"; then\n",
    "    # Environment variables and compilation for edge compute nodes with FPGAs\n",
    "    source /opt/intel/init_openvino.sh\n",
    "    aocl program acl0 /opt/intel/openvino/bitstreams/a10_vision_design_sg1_bitstreams/2019R3_PV_PL1_FP16_MobileNet_Clamp.aocx\n",
    "fi\n",
    "    \n",
    "# Running the object detection code\n",
    "python3 main.py $DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This bash script takes one argument, which specifies the device to use. \n",
    "The bit-stream is only loaded if \"FPGA\" appears in the device argument.\n",
    "\n",
    "### Job queue submission\n",
    "\n",
    "As in exercise 1, the command for submitting the job has been provided for you. \n",
    "The two main differences for this command is that it is getting the command from `job.sh` and that the argument for `job.sh` is set in by the `-F` flag of the `qsub` command. \n",
    "Once again, see the DevCloud documentation page for more information on using the `qsub` command.\n",
    "\n",
    "Additionally, the `waitForJob` function has been provided for you like in exercise 1. \n",
    "This version however, has an argument `show_stdio` to choose whether you want the stdout and stderr to be printed.\n",
    "The results of the benchmark will be printed regardless, so the stdout and stderr is primarily for your convenience in debugging if there is an issue.\n",
    "\n",
    "Run the following cell will submit the job for processing with CPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_utils import waitForJob\n",
    "job_name_cpu = !qsub job.sh -l nodes=1:skylake -F \"CPU\" -N obj_det\n",
    "print(\"Waiting on job to complete. This may take some time\")\n",
    "waitForJob(job_name_cpu, \"CPU\", show_stdio=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the run on CPU was successful, it is time to try out the other devices.\n",
    "One of the big advantage of the job qeueue system is that you can have multiple jobs running at once, so you can benchmark the remaining systems in one go.\n",
    "\n",
    "Run the following cell to run the benchmark on GPU, FPGA, and VPU.\n",
    "\n",
    "**Note:** FPGA is set to `HETERO` mode with CPU, as there are some layers that are not supported by FPGA. Also, HDDL will take longer to complete than others because it has a large optimal number of inference requests, and consequently has more images to process than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name_gpu = !qsub job.sh -l nodes=1:intel-hd-530 -F \"GPU\" -N obj_det\n",
    "job_name_fpga = !qsub job.sh -l nodes=1:iei-mustang-f100-a10 -F \"HETERO:FPGA,CPU\" -N obj_det\n",
    "job_name_hddl = !qsub job.sh -l nodes=1:iei-mustang-v100-mx8 -F \"HDDL\" -N obj_det\n",
    "print(\"Waiting on jobs to complete. This may take some time\")\n",
    "waitForJob(job_name_gpu, \"GPU\", show_stdio=False)\n",
    "waitForJob(job_name_fpga, \"HETERO:FPGA,CPU\", show_stdio=False)\n",
    "waitForJob(job_name_hddl, \"HDDL\", show_stdio=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the following cell will to get a side by side comparison of the inference time per image. \n",
    "**The quiz will ask you which device had the best (lowest) inference time per image value.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_utils import summaryPlot\n",
    "summaryPlot('results', 'Target device', 'Inference time per image (ms)', \"Inference performance (lower the better)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You now have the performance benchmark on 4 types of devices. \n",
    "Of course, these numbers are not the full story; you need consider other factors like power consumption and cost if these are important for your particuar deployment.\n",
    "But these benchmarks will be a key component in that decision making process,"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Ubuntu)",
   "language": "python",
   "name": "c003-python_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
